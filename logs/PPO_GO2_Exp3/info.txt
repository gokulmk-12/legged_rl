PPO Experiment 3

Total Timesteps: 30M
Number of parallel environments: 32

Same Observations, Reset Termination as previous 

Action Space:
    self.action_low = np.array([-0.2, 0.3, -1.9]*4)
    self.action_high = np.array([0.2, 1.15, -1.2]*4)

Rewards:
    self.reward_scales = {
        "tracking_lin_vel": 2.0,
        "tracking_ang_vel": 0.5,
        "lin_vel_z": 0.2,
        "angvel_xy": 0.2,
        "base_height": 6.0,
        "torques": 2e-4,
        "action_rate": 0.2,
        "tracking_sigma": 0.25,
        "acceleration": 2e-6,
        "feet_air_time": 0.5,
        "default_pose": 0.2,
        "orientation": 1.0,
        "energy": 5e-4,
    }

PPO Hyperparameters:
    model = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            tensorboard_log=log_dir,
            device="cuda" if torch.cuda.is_available() else "cpu",
            n_steps=2048 // num_envs,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            ent_coef=0.02,
            max_grad_norm=0.5,
            learning_rate=2e-4,
            clip_range=0.2,
            vf_coef=1.0,
            clip_range_vf=0.2
        )

What I observed ?
    * Still accelerated motion, it tried to gallop, but the pitch was signifcantly higher which led it terminate
    * Good progress, but the base stability is weak. 
    * Stable Baselines3 Nan termination is very often now
    * Seems like it drifted way from experiment 2, need to reduce some params which were increased

Changes
    * increased learning_rate to 3e-4
    * decreased acceleration scale to 5e-7
