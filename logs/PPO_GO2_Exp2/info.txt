PPO Experiment 2

Total Timesteps: 5M
Number of parallel environments: 16

Observation:
    state = np.hstack([
            noisy_linvel,
            noisy_gyro,
            noisy_gravity,
            noisy_joint_angles - self.default_joint_position,
            noisy_joint_vel,
            self.previous_action,
            self.track_commands,
    ])

    obs = np.concatenate(
        [
            state,
            gyro,
            accelerometer,
            gravity, 
            linvel,
            global_angvel,
            joint_angles - self.default_joint_position,
            joint_vel, 
            self.last_contacts,
            self.feet_air_time,
        ]
    )

Action Space:
    self.action_low = np.array([-0.3, 0.3, -1.9]*4)
    self.action_high = np.array([0.3, 1.15, -1.2]*4)

Reset Termination:
    |Roll|, |Pitch| > 15 degree, Sleep Termination < 0.2

Rewards:
    positive_rewards = self.reward_scales['tracking_lin_vel']    *linvel_track       + \
            self.reward_scales['tracking_ang_vel']               *angvel_track       + \
            self.reward_scales['feet_air_time']                  *feet_air_time
    
    negative_rewards = self.reward_scales['lin_vel_z']  *base_linvelz       + \
            self.reward_scales['angvel_xy']             *angvel_xy          + \
            self.reward_scales['base_height']           *base_height        + \
            self.reward_scales['torques']               *joint_torque       + \
            self.reward_scales['acceleration']          *joint_acceleration + \
            self.reward_scales['action_rate']           *action_smoothness  + \
            self.reward_scales['default_pose']          *default_pose       + \
            self.reward_scales['orientation']           *orientation        + \
            self.reward_scales['energy']                *energy
    
    rewards = positive_rewards * np.exp(0.02 * negative_rewards)

    self.reward_scales = {
            "tracking_lin_vel": 2.0,
            "tracking_ang_vel": 0.5,
            "lin_vel_z": 0.2,
            "angvel_xy": 0.2,
            "base_height": 6.0,
            "torques": 1e-4,
            "action_rate": 0.2,
            "tracking_sigma": 0.25,
            "acceleration": 1e-6,
            "feet_air_time": 0.5,
            "default_pose": 0.2,
            "orientation": 1.0,
            "energy": 1e-4,
        }

What I observed ?
    * Still jittery motion, it tried to gallop, but the pitch was signifcantly higher which led it terminate
    * Good progress, but the base stability has to be considered. 
    * May be increase the number of Timesteps to train

Changes
    * increasing angvel tracking scale to 1.0
    * increasing entropy coefficient in PPO to 0.03
    * increasing torque scale to 5e-4
    * increasing acceleration scale to 5e-6
    * increasing energy scale to 1e-3
    * training the environment for 10 M Timesteps